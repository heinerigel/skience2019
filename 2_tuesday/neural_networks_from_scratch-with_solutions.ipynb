{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style='background-image: url(\"../share/header_no_text.svg\") ; padding: 0px ; background-size: cover ; border-radius: 5px ; height: 250px'>\n",
    "    <div style=\"float: right ; margin: 50px ; padding: 20px ; background: rgba(255 , 255 , 255 , 0.7) ; width: 50% ; height: 150px\">\n",
    "        <div style=\"position: relative ; top: 50% ; transform: translatey(-50%)\">\n",
    "            <div style=\"font-size: xx-large ; font-weight: 900 ; color: rgba(0 , 0 , 0 , 0.8) ; line-height: 100%\">Machine Learning</div>\n",
    "            <div style=\"font-size: large ; padding-top: 20px ; color: rgba(0 , 0 , 0 , 0.5)\">A Fully Connected Neural Network From Scratch</div>\n",
    "        </div>\n",
    "    </div>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Authors:\n",
    "* Lion Krischer ([@krischer](https://github.com/krischer))\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "# We only need matplotlib and numpy here.\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---------\n",
    "\n",
    "# Step 1: Activation Function\n",
    "\n",
    "Let us first implement the activation function. For reasons of simplicity we will use the sigmoid activation function here. Recall that the formula is\n",
    "\n",
    "$$\n",
    "sigmoid(x) = \\sigma(x) = \\frac{1}{1 + e^{-x}}\n",
    "$$\n",
    "\n",
    "### Exercise A: Sigmoid Function\n",
    "\n",
    "Implement this function and plot it in the interval $[-10, 10]$. Please use `numpy`'s `np.exp()` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    return 1.0 / (1.0 + np.exp(-x))\n",
    "\n",
    "x = np.linspace(-10, 10, 1000)\n",
    "plt.plot(x, sigmoid(x));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "\n",
    "# Step 2: Forward Pass\n",
    "\n",
    "The goal is to implement this simple neural network from the lecture. We will later on use this to approximate a simple function of one variable. The first step is to code up the forward pass.\n",
    "\n",
    "![Simple network](images/simple_network.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise B: Single Layer\n",
    "\n",
    "For now let us focus on the middle part:\n",
    "\n",
    "![Simple network middle](images/simple_network_middle.png)\n",
    "\n",
    "Given sets of inputs, $x_1$, $x_2$, and $x_3$, and weights $\\omega_i, i=1...6$, compute $y_1$ and $y_2$. Remember to also apply the sigmoid function to the outputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = [-0.5, 0.1, 0.3]\n",
    "w = [0.0, 1.0, -2.0, 3.0, -4.0, 5.0]\n",
    "\n",
    "y1 = sigmoid(x[0] * w[0] + x[1] * w[2] + x[2] * w[4])\n",
    "y2 = sigmoid(x[0] * w[1] + x[1] * w[3] + x[2] * w[5])\n",
    "\n",
    "print(y1, y2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You probably realized that this is suspiciously similar to a Matrix multiplication. If you have not already done so, rewrite the whole operation as a matrix multiplication. Remember that you need to conver the lists to `numpy` arrays. To perform a matrix multiplication use the `@` operator, e.g. `A @ B` will compute the matrix multiplication of `A` and `B`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "W = np.array(w).reshape((3, 2)).T\n",
    "sigmoid(W @ x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise C: Full Forward Pass\n",
    "\n",
    "Now implement the full forward pass - do it without bias units for now.\n",
    "\n",
    "To get comparabale results, please use an input of 0.5 and intialize all weights to 2.0 (use `np.ones(shape) * 2.0`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = [0.5]\n",
    "\n",
    "# Initialize the weights. Writing it down\n",
    "# should convince you that the first dimension\n",
    "# must be equal to the number of outputs and\n",
    "# the second equal to the number of inputs.\n",
    "weights_1 = np.ones((3, 1)) * 2.0\n",
    "weights_2 = np.ones((2, 3)) * 2.0\n",
    "weights_3 = np.ones((1, 2)) * 2.0\n",
    "\n",
    "# Now just apply all the weights and\n",
    "# the activation function.\n",
    "at_hidden_1 = sigmoid(weights_1 @ inputs)\n",
    "at_hidden_2 = sigmoid(weights_2 @ at_hidden_1)\n",
    "# No activation function for the last layer!\n",
    "output = weights_3 @ at_hidden_2\n",
    "\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 3: A Simple Neural Networks Framework\n",
    "\n",
    "Two lessons to draw from this so far:\n",
    "\n",
    "* Fully connected layers can be computed by dense matrix multiplications. This also happens to be one of the reasons why it is so fast on GPUs.\n",
    "* When actually performing the computations a \"layer\" are not the actual neurons but the operations that happen in between one set of neurons and the next.\n",
    "\n",
    "The following is a pre-made implementation of the forward pass of the desired neural network. Note that it is very similar to what we just did, there is just some bookkeeping around it and a slightly nicer structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FullyConnectedLayer:\n",
    "    def __init__(self,\n",
    "                 # The type hints after the colon are a newish Python future\n",
    "                 # and fully optional. But they document the expected type\n",
    "                 # and thus help clarify the code.\n",
    "                 input_size: int,\n",
    "                 output_size: int,\n",
    "                 bias_units: bool,\n",
    "                 activation_function=None):\n",
    "        \"\"\"\n",
    "        This function is called upon object creation.\n",
    "        \"\"\"\n",
    "        # Initialize the weights. The matrix must always be\n",
    "        # an M x (N + 1) matrix. M rows, one for each output, and\n",
    "        # N + 1 columns, one for each input + one for the bias.\n",
    "        #\n",
    "        # Here we just initialize with a normal distribution with\n",
    "        # zero mean and a standard deviation of 0.1. Small initial\n",
    "        # weights results in faster initial training!\n",
    "        self.bias_units = bias_units\n",
    "        if bias_units:\n",
    "            i_size = input_size + 1\n",
    "        else:\n",
    "            i_size = input_size\n",
    "        self.weights = np.random.randn(output_size, i_size) * 0.1\n",
    "        \n",
    "        # Also set the activation function.\n",
    "        self.activation_function = activation_function\n",
    "        \n",
    "\n",
    "    def forward(self, inputs: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        This function performs the forward pass for that layer given\n",
    "        a set of inputs.\n",
    "        \"\"\"\n",
    "        # Add the bias at the end of the inputs.\n",
    "        if self.bias_units:\n",
    "            inputs = np.pad(inputs, pad_width=((0, 0), (0, 1)),\n",
    "                            mode=\"constant\", constant_values=(0.0, 1.0))\n",
    "            \n",
    "        # Keep the inputs around. We need them for the backwards pass.\n",
    "        self._forward_inputs = inputs\n",
    "        \n",
    "        # Perform the matrix multiplication and apply the\n",
    "        # activation function.\n",
    "        #\n",
    "        # Not that we flip the sign of operations here to\n",
    "        # be able to treat the first dimension as the \"sample\"\n",
    "        # dimension.\n",
    "        out = inputs @ self.weights.T\n",
    "        \n",
    "        # Last but not least apply the activation function.\n",
    "        if self.activation_function:\n",
    "            out = self.activation_function(out)\n",
    "            \n",
    "        # We also need the outputs for the backwards run.\n",
    "        self._forward_outputs = out\n",
    "        \n",
    "        return out\n",
    "    \n",
    "    \n",
    "class NeuralNetwork:\n",
    "    def __init__(self):\n",
    "        self.layers = []\n",
    "    \n",
    "    def add_layer(self, layer: FullyConnectedLayer):\n",
    "        self.layers.append(layer)\n",
    "        \n",
    "    def forward(self, inputs: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Just apply the forward operator of each layer.\n",
    "        \"\"\"\n",
    "        inputs = np.asarray(inputs)\n",
    "        out = self.layers[0].forward(inputs)\n",
    "        for l in self.layers[1:]:\n",
    "            out = l.forward(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now remember that each \"layer\" denotes the connections\n",
    "# between two sets of layers.\n",
    "fc1 = FullyConnectedLayer(input_size=1, output_size=3,\n",
    "                          bias_units=False,\n",
    "                          activation_function=sigmoid)\n",
    "fc2 = FullyConnectedLayer(input_size=3, output_size=2,\n",
    "                          bias_units=False,\n",
    "                          activation_function=sigmoid)\n",
    "fc3 = FullyConnectedLayer(input_size=2, output_size=1, \n",
    "                          bias_units=False)\n",
    "\n",
    "\n",
    "# Add each layer to the neural network.\n",
    "nn = NeuralNetwork()\n",
    "nn.add_layer(fc1)\n",
    "nn.add_layer(fc2)\n",
    "nn.add_layer(fc3)\n",
    "\n",
    "# Now we do a bit of a hack to get the same result\n",
    "# as previously.\n",
    "fc1.weights[:] = 2.0\n",
    "fc2.weights[:] = 2.0\n",
    "fc3.weights[:] = 2.0\n",
    "\n",
    "# Our input data has to have the shape\n",
    "# (NUMBER_OF_SAMPLES, POINTS PER SAMPLE)\n",
    "inputs = np.array([0.5]).reshape(1, 1)\n",
    "print(nn.forward(inputs))\n",
    "\n",
    "# This enables us to perform the operation\n",
    "# for many data samples at once.\n",
    "inputs = np.array([[0.5],\n",
    "                   [-10.0],\n",
    "                   [0.5],\n",
    "                   [0.0]])\n",
    "print(nn.forward(inputs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 4: Back-propagation\n",
    "\n",
    "The first thing we need it so find the derivative through whatever activation function we chose."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x, derivative=False):\n",
    "    # Not the true derivative but the derivative,\n",
    "    # assuming x is already sigmoid(value).\n",
    "    if derivative:\n",
    "        return x * (1 - x)\n",
    "    return 1.0 / (1.0 + np.exp(-x))\n",
    "\n",
    "x = np.linspace(-10, 10, 1000)\n",
    "plt.plot(x, sigmoid(x));\n",
    "plt.show()\n",
    "plt.plot(x, sigmoid(sigmoid(x), derivative=True));\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we implement the backwards pass on the previously already explain fully connected layer class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FullyConnectedLayer:\n",
    "    def __init__(self,\n",
    "                 # The type hints after the colon are a newish Python future\n",
    "                 # and fully optional. But they document the expected type\n",
    "                 # and thus help clarify the code.\n",
    "                 input_size: int,\n",
    "                 output_size: int,\n",
    "                 bias_units: bool,\n",
    "                 activation_function=None):\n",
    "        \"\"\"\n",
    "        This function is called upon object creation.\n",
    "        \"\"\"\n",
    "        # Initialize the weights. The matrix must always be\n",
    "        # an M x (N + 1) matrix. M rows, one for each output, and\n",
    "        # N + 1 columns, one for each input + one for the bias.\n",
    "        #\n",
    "        # Here we just initialize with a normal distribution with\n",
    "        # zero mean and a standard deviation of 0.1. Small initial\n",
    "        # weights results in faster initial training!\n",
    "        self.bias_units = bias_units\n",
    "        if bias_units:\n",
    "            i_size = input_size + 1\n",
    "        else:\n",
    "            i_size = input_size\n",
    "        self.weights = np.random.randn(output_size, i_size) * 0.1\n",
    "        \n",
    "        # Also set the activation function.\n",
    "        self.activation_function = activation_function\n",
    "        \n",
    "\n",
    "    def forward(self, inputs: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        This function performs the forward pass for that layer given\n",
    "        a set of inputs.\n",
    "        \"\"\"\n",
    "        # Add the bias at the end of the inputs.\n",
    "        if self.bias_units:\n",
    "            inputs = np.pad(inputs, pad_width=((0, 0), (0, 1)),\n",
    "                            mode=\"constant\", constant_values=(0.0, 1.0))\n",
    "            \n",
    "        # Keep the inputs around. We need them for the backwards pass.\n",
    "        self._forward_inputs = inputs\n",
    "        \n",
    "        # Perform the matrix multiplication and apply the\n",
    "        # activation function.\n",
    "        #\n",
    "        # Not that we flip the sign of operations here to\n",
    "        # be able to treat the first dimension as the \"sample\"\n",
    "        # dimension.\n",
    "        out = inputs @ self.weights.T\n",
    "        \n",
    "        # Last but not least apply the activation function.\n",
    "        if self.activation_function:\n",
    "            out = self.activation_function(out)\n",
    "            \n",
    "        # We also need the outputs for the backwards run.\n",
    "        self._forward_outputs = out\n",
    "        \n",
    "        return out\n",
    "        \n",
    "    def backward(self, g: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Backpropagate any passed gradients and store the\n",
    "        gradients with respect to the weights.\n",
    "        \"\"\"\n",
    "        # Backprop through the activation function if any.\n",
    "        if self.activation_function:\n",
    "            g = self.activation_function(\n",
    "                self._forward_outputs, derivative=True) * g\n",
    "            \n",
    "        # Store the gradients as it is needed for the\n",
    "        # optimization pass.\n",
    "        self._gradient_weights = g.T @ self._forward_inputs\n",
    "        \n",
    "        # Apply the weights for the back-propagation to the\n",
    "        # previous layer.\n",
    "        bp = g @ self.weights\n",
    "        \n",
    "        if self.bias_units:\n",
    "            # No need to back-propagate the bias weights.\n",
    "            return bp[:, :-1]\n",
    "        else:\n",
    "            return bp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And our neural network class needs to gain the ability to perform a backwards pass."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetwork:\n",
    "    def __init__(self):\n",
    "        self.layers = []\n",
    "    \n",
    "    def add_layer(self, layer: FullyConnectedLayer):\n",
    "        self.layers.append(layer)\n",
    "        \n",
    "    def forward(self, inputs: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Just apply the forward operator of each layer.\n",
    "        \"\"\"\n",
    "        inputs = np.asarray(inputs)\n",
    "        out = self.layers[0].forward(inputs)\n",
    "        for l in self.layers[1:]:\n",
    "            out = l.forward(out)\n",
    "        return out\n",
    "    \n",
    "    def backward(self, outputs: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Apply the backwards pass of each layer.\n",
    "        \"\"\"\n",
    "        g = np.asarray(outputs)\n",
    "        g = self.layers[-1].backward(g)\n",
    "        for l in reversed(self.layers[:-1]):\n",
    "            g = l.backward(g)\n",
    "        return g"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now demonstrate the usage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now remember that each \"layer\" denotes the connections\n",
    "# between two sets of layers.\n",
    "fc1 = FullyConnectedLayer(input_size=1, output_size=3,\n",
    "                          bias_units=True,\n",
    "                          activation_function=sigmoid)\n",
    "fc2 = FullyConnectedLayer(input_size=3, output_size=2,\n",
    "                          bias_units=True,\n",
    "                          activation_function=sigmoid)\n",
    "fc3 = FullyConnectedLayer(input_size=2, output_size=1, \n",
    "                          bias_units=True)\n",
    "\n",
    "\n",
    "# Add each layer to the neural network.\n",
    "nn = NeuralNetwork()\n",
    "nn.add_layer(fc1)\n",
    "nn.add_layer(fc2)\n",
    "nn.add_layer(fc3)\n",
    "\n",
    "\n",
    "# Our input data has to have the shape\n",
    "# (NUMBER_OF_SAMPLES, POINTS PER SAMPLE)\n",
    "inputs = np.array([0.5]).reshape(1, 1)\n",
    "print(nn.backward(nn.forward(inputs)))\n",
    "\n",
    "# This enables us to perform the operation\n",
    "# for many data samples at once.\n",
    "inputs = np.array([[0.5],\n",
    "                   [-10.0],\n",
    "                   [0.5],\n",
    "                   [0.0]])\n",
    "print(nn.backward(nn.forward(inputs)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 5: Gradient Descent\n",
    "\n",
    "### Exercise D: Perform gradient descent on $f(x) = x^2$\n",
    "\n",
    "or $f(x, y) = x^2 y^2$, $f(x) = sin(x)$, or some other function of choice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Starting point.\n",
    "x = -4\n",
    "step_length = 0.1\n",
    "niter = 100\n",
    "\n",
    "for _ in range(niter):\n",
    "    grad_x = 2 * x\n",
    "    x += -grad_x * step_length\n",
    "    \n",
    "print(x, x**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Starting point.\n",
    "x = -4\n",
    "y = 3.5\n",
    "\n",
    "step_length = 0.01\n",
    "niter = 1000\n",
    "\n",
    "for _ in range(niter):\n",
    "    grad_x = 2 * x * y ** 2\n",
    "    grad_y = 2 * y * x ** 2\n",
    "    x += -grad_x * step_length\n",
    "    y += -grad_y * step_length\n",
    "\n",
    "print(x, y, x ** 2 * y ** 2)\n",
    "\n",
    "# Plot the function.\n",
    "X, Y = np.meshgrid(np.linspace(10, -10, 200),\n",
    "                   np.linspace(10, -10, 200))\n",
    "plt.pcolormesh(X, Y, X ** 2 * Y ** 2)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 6:  Synthesis\n",
    "\n",
    "Tie everything together.\n",
    "\n",
    "We will define a function here, draw some samples from that function and attempt to train a neural network to reproduce that function using the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def function(x):\n",
    "    return np.sin(np.pi * x) / 2.0 + 0.5\n",
    "\n",
    "\n",
    "# Training data. N samples uniformly distributed\n",
    "# in the entire range.\n",
    "N = 100\n",
    "X = np.random.random((N, 1)) * 2.0\n",
    "Y = function(X)\n",
    "\n",
    "\n",
    "x = np.linspace(0, 2, 1000)\n",
    "plt.plot(x, function(x), label=\"True\")\n",
    "plt.scatter(X, Y, color=\"red\", label=\"Training Data\")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following will be a copy of our neural network classes from before with a few subtle changes:\n",
    "\n",
    "* The `NeuralNetwork` class now has the ability to compute a squared error loss function.\n",
    "* The `FullyConnectedLayer` can update its weights using the gradients from the backwards pass.\n",
    "* The actual optimization is performed outside of these classes for reasons of clarity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "class FullyConnectedLayer:\n",
    "    def __init__(self,\n",
    "                 # The type hints after the colon are a newish Python future\n",
    "                 # and fully optional. But they document the expected type\n",
    "                 # and thus help clarify the code.\n",
    "                 input_size: int,\n",
    "                 output_size: int,\n",
    "                 bias_units: bool,\n",
    "                 activation_function=None):\n",
    "        \"\"\"\n",
    "        This function is called upon object creation.\n",
    "        \"\"\"\n",
    "        # Initialize the weights. The matrix must always be\n",
    "        # an M x (N + 1) matrix. M rows, one for each output, and\n",
    "        # N + 1 columns, one for each input + one for the bias.\n",
    "        #\n",
    "        # Here we just initialize with a normal distribution with\n",
    "        # zero mean and a standard deviation of 0.1. Small initial\n",
    "        # weights results in faster initial training!\n",
    "        self.bias_units = bias_units\n",
    "        if bias_units:\n",
    "            i_size = input_size + 1\n",
    "        else:\n",
    "            i_size = input_size\n",
    "        self.weights = np.random.randn(output_size, i_size) * 0.1\n",
    "        \n",
    "        # Also set the activation function.\n",
    "        self.activation_function = activation_function\n",
    "        \n",
    "\n",
    "    def forward(self, inputs: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        This function performs the forward pass for that layer given\n",
    "        a set of inputs.\n",
    "        \"\"\"\n",
    "        # Add the bias at the end of the inputs.\n",
    "        if self.bias_units:\n",
    "            inputs = np.pad(inputs, pad_width=((0, 0), (0, 1)),\n",
    "                            mode=\"constant\", constant_values=(0.0, 1.0))\n",
    "            \n",
    "        # Keep the inputs around. We need them for the backwards pass.\n",
    "        self._forward_inputs = inputs\n",
    "        \n",
    "        # Perform the matrix multiplication and apply the\n",
    "        # activation function.\n",
    "        #\n",
    "        # Not that we flip the sign of operations here to\n",
    "        # be able to treat the first dimension as the \"sample\"\n",
    "        # dimension.\n",
    "        out = inputs @ self.weights.T\n",
    "        \n",
    "        # Last but not least apply the activation function.\n",
    "        if self.activation_function:\n",
    "            out = self.activation_function(out)\n",
    "            \n",
    "        # We also need the outputs for the backwards run.\n",
    "        self._forward_outputs = out\n",
    "        \n",
    "        return out\n",
    "        \n",
    "    def backward(self, g: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Backpropagate any passed gradients and store the\n",
    "        gradients with respect to the weights.\n",
    "        \"\"\"\n",
    "        # Backprop through the activation function if any.\n",
    "        if self.activation_function:\n",
    "            g = self.activation_function(\n",
    "                self._forward_outputs, derivative=True) * g\n",
    "            \n",
    "        # Store the gradients as it is needed for the\n",
    "        # optimization pass.\n",
    "        self._gradient_weights = g.T @ self._forward_inputs\n",
    "        \n",
    "        # Apply the weights for the back-propagation to the\n",
    "        # previous layer.\n",
    "        bp = g @ self.weights\n",
    "        \n",
    "        if self.bias_units:\n",
    "            # No need to back-propagate the bias weights.\n",
    "            return bp[:, :-1]\n",
    "        else:\n",
    "            return bp\n",
    "        \n",
    "    def apply_negative_gradients(self, step_length: float):\n",
    "        \"\"\"\n",
    "        Applies the negative gradient to the weights with a given\n",
    "        step length.\n",
    "        \"\"\"\n",
    "        self.weights -= step_length * self._gradient_weights\n",
    "        \n",
    "        \n",
    "class NeuralNetwork:\n",
    "    def __init__(self):\n",
    "        self.layers = []\n",
    "    \n",
    "    def add_layer(self, layer: FullyConnectedLayer):\n",
    "        self.layers.append(layer)\n",
    "        \n",
    "    def forward(self, inputs: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Just apply the forward operator of each layer.\n",
    "        \"\"\"\n",
    "        inputs = np.asarray(inputs)\n",
    "        out = self.layers[0].forward(inputs)\n",
    "        for l in self.layers[1:]:\n",
    "            out = l.forward(out)\n",
    "            \n",
    "        return out\n",
    "    \n",
    "    def backward(self, outputs: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Apply the backwards pass of each layer.\n",
    "        \"\"\"\n",
    "        g = np.asarray(outputs)\n",
    "        g = self.layers[-1].backward(g)\n",
    "        for l in reversed(self.layers[:-1]):\n",
    "            g = l.backward(g)\n",
    "        return g\n",
    "    \n",
    "    def loss(self,\n",
    "             actual: np.ndarray,\n",
    "             predicted: np.ndarray,\n",
    "             derivative: bool = False):\n",
    "        \"\"\"\n",
    "        Compute the squared error loss or the derivative\n",
    "        thereof.\n",
    "        \"\"\"\n",
    "        if derivative is False:\n",
    "            return 0.5 * ((actual - predicted) ** 2).sum()\n",
    "        # For the derivative we need it per sample and\n",
    "        # output.\n",
    "        return (actual - predicted)\n",
    "    \n",
    "    def apply_negative_gradients(self, step_length: float):\n",
    "        \"\"\"\n",
    "        Apply the negative gradients of each layer.\n",
    "        \"\"\"\n",
    "        for l in self.layers:\n",
    "            l.apply_negative_gradients(step_length)\n",
    "    \n",
    "\n",
    "# Now remember that each \"layer\" denotes the connections\n",
    "# between two sets of layers.\n",
    "fc1 = FullyConnectedLayer(input_size=1, output_size=3,\n",
    "                          bias_units=True,\n",
    "                          activation_function=sigmoid)\n",
    "fc2 = FullyConnectedLayer(input_size=3, output_size=2,\n",
    "                          bias_units=True,\n",
    "                          activation_function=sigmoid)\n",
    "fc3 = FullyConnectedLayer(input_size=2, output_size=1, \n",
    "                          bias_units=True)\n",
    "\n",
    "\n",
    "# Add each layer to the neural network.\n",
    "nn = NeuralNetwork()\n",
    "nn.add_layer(fc1)\n",
    "nn.add_layer(fc2)\n",
    "nn.add_layer(fc3)\n",
    "\n",
    "\n",
    "# Perform the numerical optimization.\n",
    "niter = 10000\n",
    "step_length = 0.01\n",
    "\n",
    "\n",
    "# Loop over iterations.\n",
    "for i in range(niter):\n",
    "    # Perform the forward pass.\n",
    "    predicted = nn.forward(X)\n",
    "    \n",
    "    if not i % 1000:\n",
    "        print(f\"Iteration {i}: Loss: {nn.loss(predicted, Y)}\")\n",
    "    \n",
    "    # Play a bit with an adaptive step length.\n",
    "    if i == 6000:\n",
    "        step_length /= 2.0\n",
    "    \n",
    "    # Compute the derivative of the loss.\n",
    "    d_l = nn.loss(predicted, Y, derivative=True)\n",
    "    # Backpropagate it to store the gradients with respect\n",
    "    # to the weights in each layer.\n",
    "    nn.backward(d_l)\n",
    "    # Update each layer and rinse and repeat.\n",
    "    nn.apply_negative_gradients(step_length=step_length)\n",
    "    \n",
    "\n",
    "print(f\"Iteration {i}: Loss: {nn.loss(predicted, Y)}\") \n",
    "    \n",
    "# Print some predictions.\n",
    "x = np.linspace(0, 2, 1000)\n",
    "x = x.reshape(-1, 1)\n",
    "plt.plot(x, function(x), label=\"true\")\n",
    "plt.plot(x, nn.forward(x), label=\"learned\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 7: Don'y be silly: Use a Library!\n",
    "\n",
    "You might have noticed that there are many steps where one can slighty go wrong. Thankfully good libraries for machine learning and neural networks are in existence. Do yourself a favor and use them! The following block contains a recreation of this whole notebook using two calls two to `scikit-learn`. On Thursday and Friday you will meet other, even more powerful, neural network frameworks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPRegressor\n",
    "\n",
    "# Define the network.\n",
    "nn = MLPRegressor(\n",
    "    hidden_layer_sizes=(3, 2),\n",
    "    solver=\"sgd\",\n",
    "    # No batches.\n",
    "    batch_size=N,\n",
    "    # Logistic == sigmoid\n",
    "    activation=\"logistic\",\n",
    "    # Fairly fragile to these settings here.\n",
    "    learning_rate_init=0.2,\n",
    "    learning_rate=\"adaptive\",\n",
    "    tol=1E-7,\n",
    "    n_iter_no_change=100,\n",
    "    max_iter=10000,\n",
    "    # No regularization.\n",
    "    alpha=0.0,\n",
    "    # No validation set.\n",
    "    validation_fraction=0.0)\n",
    "\n",
    "# Learn it.\n",
    "f = nn.fit(X, Y.ravel())\n",
    "print(f\"Final Loss: {f.loss_}\")\n",
    "print(f\"Number of Iterations: {f.n_iter_}\")\n",
    "# Print some predictions.\n",
    "x = np.linspace(0, 2, 1000)\n",
    "x = x.reshape(-1, 1)\n",
    "plt.plot(x, function(x), label=\"true\")\n",
    "plt.plot(x, nn.predict(x), label=\"learned\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
