{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "![Randomforest](figs/rf_title.png)\n",
    "\n",
    "Author: Michaela Wenner (wenner@vaw.baug.ethz.ch) \n",
    "<p style=\"text-align: right\"> \n",
    "    <a href=\"https://medium.com/tech-vision/random-forest-classification-with-h2o-python-for-beginners-b31f6e4ccf3c\">Figure Source</a> </p>\n",
    "\n",
    "Literature:\n",
    "* James, G., Witten, D., Hastie, T. and Tibshirani, R., 2013. [An introduction to statistical learning (Vol. 112)](https://www-bcf.usc.edu/~gareth/ISL/ISLR%20First%20Printing.pdf). New York: springer. \n",
    "* Breiman, L., 2017. [Classification and regression trees](https://www.taylorfrancis.com/books/9781351460491). Routledge.\n",
    "* Breiman, L., 2001. [Random forests](https://link.springer.com/article/10.1023/A:1010933404324). Machine learning, 45(1), pp.5-32.\n",
    "\n",
    "Tutorials:\n",
    "* [Introduction](https://github.com/WillKoehrsen/Machine-Learning-Projects/blob/master/Random%20Forest%20Tutorial.ipynb)\n",
    "* [In-Depth: Decision Trees and Random Forests](https://jakevdp.github.io/PythonDataScienceHandbook/05.08-random-forests.html)\n",
    "* [Parameter Tuning](https://towardsdatascience.com/hyperparameter-tuning-the-random-forest-in-python-using-scikit-learn-28d2aa77dd74)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "# In this notebook\n",
    "\n",
    "Here, we introduce decision trees and random forests as classifiers and make use of the algorithms to classify a handpicked catalog of seismic data. We will learn how a decision tree is build and what parameters can be changed to improve test results. Additionally, we are going to have a look at how ensemble methods can improve classification results and apply Random Forest to actual seismic data.\n",
    "\n",
    "### Outline:\n",
    "\n",
    "* Decision trees as classifier\n",
    "* Ensemble methods: Random Forest\n",
    "* Classification of seismic signals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [],
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "# Configuration setup (run before doing anything else)\n",
    "\n",
    "%matplotlib notebook\n",
    "#Loading packages\n",
    "import obspy\n",
    "from obspy import UTCDateTime\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import seaborn as sns\n",
    "from ipywidgets import interact, interactive, fixed, interact_manual\n",
    "import ipywidgets as widgets\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "sns.set_style('white')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "<img align=\"right\" src=\"figs/dec_trees.png\" width=\"500\"> \n",
    "\n",
    "# Decision trees\n",
    "\n",
    "\n",
    "Classification trees are **non-linear**, **non-parametric** classifiers that predict a **quantitative** response. Hence, the predictor space is divided into distinct non overlapping regions, each representing one class.\n",
    "\n",
    "Each observation is than predicted to belong to one region. The class of the region is determined by the training data set.\n",
    "\n",
    "\n",
    "\n",
    "### Terminology\n",
    "\n",
    "\n",
    "\n",
    "**Nodes** \n",
    "\n",
    "-> Test for variables\n",
    "\n",
    "**Leaves**\n",
    "\n",
    "-> Classification result\n",
    "\n",
    "*Fig. 1: From Introduction to Statistical Learning (a) Partition of feature space, not possible bin binary splitting (b) Example for feature space by binary splitting (c) Example of a decision tree for regions shown in b (d) 3rd dimension showing the \"prediction surface\"*\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification trees - Criterion for binary splits\n",
    "\n",
    "\n",
    "### Classification error rate\n",
    "-> fraction of training samples that do not belong to most common class in region\n",
    "\n",
    "$$E = 1- \\max_{k} \\hat{p}_{mk}$$\n",
    "\n",
    "\n",
    "$\\hat{p}_{mk} $ = proportion of training observations in the mth region that are from the kth class\n",
    "\n",
    "### Gini index\n",
    "-> mesure of **total variance** across k-classes\n",
    "\n",
    "$$G = \\sum_{k=1}^{k} \\hat{p}_{mk} (1-\\hat{p}_{mk})$$\n",
    "\n",
    "\n",
    "### Entropy\n",
    "$$D = - \\sum_{k=1}^{k} \\hat{p}_{mk} log(\\hat{p}_{mk})$$\n",
    "\n",
    "Since $0 \\leq \\hat{p}_{mk} \\leq 1$ it follows that $0 \\leq -\\hat{p}_{mk} log(\\hat{p}_{mk})$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "# Plot classification error rate, gini index and entropy\n",
    "\n",
    "def class_error(p):\n",
    "     return 1 - np.max([p, 1-p])\n",
    "    \n",
    "def gini_index(p):\n",
    "    return p * (1-p) + (1-p) * (p)\n",
    "\n",
    "def entropy(p):\n",
    "    return - (p * np.log2(p) + (1-p)*np.log2(1-p))\n",
    "\n",
    "x = np.arange(0.0, 1.0, 0.01)\n",
    "x_ent = np.arange(0.01, 0.99, 0.01)\n",
    "c_err = [class_error(i) for i in x]\n",
    "\n",
    "fig = plt.figure()\n",
    "plt.plot(x_ent, entropy(x_ent)/2, label = 'Entropy (scaled)')\n",
    "plt.plot(x, gini_index(x), label = 'Gini index')\n",
    "plt.plot(x, c_err, label = 'Classification error')\n",
    "plt.xlabel('p')\n",
    "plt.ylabel('impurity')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<img align=\"right\" src=\"figs/purity.png\" width=\"400\"> \n",
    "\n",
    "\n",
    "# Node purity\n",
    "\n",
    "\n",
    "\n",
    "= Node contains predominantly **observations from a single class**\n",
    "\n",
    "* Increases uncertainty for some predictions - decreases uncertainty for others\n",
    "\n",
    "* Does not reduce the classification error\n",
    "\n",
    "-> **Gini index** and **entropy** more sensitive to node purity\n",
    "\n",
    "*Fig 2: regardless of the feature being >0.5 or not, the outcome will always be yes.*\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "# Classification Trees - basic example\n",
    "\n",
    "* set of seismic events with different features\n",
    "\n",
    "\n",
    "* two class problem: earthquake - noise (simplicity)\n",
    "\n",
    "\n",
    "* two features (e.g. length, dominant frequency)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "# Read csv file with features as first two column and event class as last column\n",
    "columns = ['feature1', 'feature2','event_class']\n",
    "\n",
    "# Read to data frame\n",
    "df = pd.read_csv('material/table_decisiontree.csv', names=columns, dtype={'event_class':np.int32})\n",
    "\n",
    "# Add event names to pandas data frame - also called qualitative/categorical/discrete variables\n",
    "event_names = np.array(['noise', 'earthquake'])\n",
    "tar = np.array(df['event_class'])\n",
    "df['event'] = pd.Categorical.from_codes(tar, event_names)\n",
    "\n",
    "# seperate noise and earthquake events dataframe\n",
    "df_noise = df[df['event'] == 'noise']\n",
    "df_eq = df[df['event'] == 'earthquake']\n",
    "\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [],
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "# Define colors and cmap for data visualisation\n",
    "colors = sns.diverging_palette(220, 20, n=2)\n",
    "cmap = sns.diverging_palette(220, 20, center='dark', as_cmap=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "# Plot event features on two dimensional scatter plot\n",
    "fig = plt.figure()\n",
    "plt.scatter(df_noise['feature1'], df_noise['feature2'], color=colors[0], s = 1000,\\\n",
    "            marker=f\"$ {df_noise['event'].iloc[0]} $\")\n",
    "plt.scatter(df_eq['feature1'], df_eq['feature2'], color=colors[1], s = 4000,\\\n",
    "            marker=f\"$ {df_eq['event'].iloc[0]} $\")\n",
    "plt.ylabel('feat2')\n",
    "plt.xlabel('feat1')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [],
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "# Creat array of features and labels\n",
    "features = df.columns[0:2]\n",
    "X = np.asarray(df[features])\n",
    "Y = np.asarray(df['event_class'])\n",
    "\n",
    "print(f'Array of features \\n {X}')\n",
    "print(f'Array of labels \\n {Y}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [],
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "def visualize_classifier(classifier, X, Y, cmap=cmap):\n",
    "    fig, ax = plt.subplots()\n",
    "    \n",
    "    plt.scatter(X[:, 0], X[:, 1], c=Y, s=50, cmap=cmap)\n",
    "\n",
    "    xlim = ax.get_xlim()\n",
    "    ylim = ax.get_ylim()\n",
    "    \n",
    "    xx, yy = np.meshgrid(np.linspace(*xlim, num=200),\n",
    "                         np.linspace(*ylim, num=200))\n",
    "    Z = classifier.predict(np.c_[xx.ravel(), yy.ravel()]).reshape(xx.shape)\n",
    "\n",
    "    # Create a color plot with classified areas\n",
    "    n_classes = len(np.unique(Y))\n",
    "    contours = ax.contourf(xx, yy, Z, alpha=0.3, levels=np.arange(n_classes + 1) - 0.5, cmap=cmap) \n",
    "    ax.contour(contours, colors= 'gray')\n",
    "\n",
    "    ax.set(xlim=xlim, ylim=ylim)\n",
    "    ax.set_xlabel('feat1',fontsize=12)\n",
    "    ax.set_ylabel('feat2',fontsize=12)\n",
    "    ax.tick_params(axis='both', labelsize=12)\n",
    "    \n",
    "    return fig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Scikit-learn DecisionTreeClassifier](https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "# Define classifier\n",
    "clf = DecisionTreeClassifier(criterion='entropy', max_depth=None, min_samples_split=2, min_samples_leaf=1, \\\n",
    "                             max_leaf_nodes=None)\n",
    "\n",
    "# Fit estimator on training data set\n",
    "model = clf.fit(X,Y)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "visualize_classifier(model, X, Y);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [],
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "# Visualize decision tree\n",
    "import graphviz\n",
    "from sklearn import tree\n",
    "from subprocess import call\n",
    "from IPython.display import Image\n",
    "\n",
    "dot_data = tree.export_graphviz(clf, out_file='figs/tree.dot', feature_names=['feat1', 'feat2'], \\\n",
    "                                class_names=['noise', 'earthquake'], filled=True, rounded=True,)\n",
    "\n",
    "# Convert decision tree to png\n",
    "call(['dot', '-Tpng', 'figs/tree.dot', '-o', 'figs/tree.png', '-Gdpi=400']);\n",
    "\n",
    "# Display decision tree\n",
    "Image('figs/tree.png', width=400)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "# Use blob data set for more realistic problem\n",
    "\n",
    "from sklearn.datasets import make_blobs\n",
    "\n",
    "# Define function to create data set of sample size n_samples\n",
    "# Number of classes defined by centers\n",
    "# Standard deviation of blobs in each class defined by cluster_std\n",
    "# Creates interactive plot to visualize feature space for datasets with different std\n",
    "def inter_tree(std):\n",
    "    X, Y = make_blobs(n_samples=400, centers=3,\n",
    "                  random_state=0, cluster_std=std)\n",
    "    model = clf.fit(X,Y)\n",
    "    visualize_classifier(model, X, Y)\n",
    "\n",
    "interact(inter_tree, std=(0.0,2.0));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = [[0,1,0]]\n",
    "class_names = ['blue', 'black', 'red']\n",
    "for idx, cl in enumerate(a[0]):\n",
    "    if cl == 1:\n",
    "        print(class_names[idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [],
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "# Predicts probabilities for a give set of features\n",
    "def pred_pro(feat1,feat2):\n",
    "    class_names = ['blue', 'black', 'red']\n",
    "    pred_proba = clf.predict_proba([[feat1,feat2]])\n",
    "    \n",
    "    [print(\"Predicted class:\", class_names[idx]) for idx, cl in enumerate(pred_proba[0]) if cl == 1.]\n",
    "    X, Y = make_blobs(n_samples=400, centers=3,\n",
    "                  random_state=0, cluster_std=1)\n",
    "    model = clf.fit(X,Y)\n",
    "    visualize_classifier(model, X, Y)\n",
    "    plt.plot(feat1, feat2, '*', color='yellow', markersize=15)\n",
    "    \n",
    "interact(pred_pro, feat1=(-4.0,4.0), feat2=(-4.0, 4.0));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.close('all')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decision trees\n",
    "\n",
    "| <p align=\"left.\">PROS                         | CONS                         |\n",
    "| --------------------------------------------- | ---------------------------- |\n",
    "| <p align=\"left.\">easy to understand           | predictive accuracy          |\n",
    "| <p align=\"left.\">can be graphically displayed | high variance                |   \n",
    "| <p align=\"left.\">computationally inexpensive  |                              |\n",
    "\n",
    "\n",
    "\n",
    "Make use of PROS but get rid of CONS:\n",
    "\n",
    "=> **Ensemble Methods**\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ensemble Methods\n",
    "\n",
    "Combine many (high complexity, low bias) models to reduce variane\n",
    "\n",
    "### As a reminder\n",
    "\n",
    "* We want to estimate some characteristic of a **probability distribution P**\n",
    "\n",
    "  -> **Parameter** of P\n",
    "  \n",
    "  e.g. expected value, variance, kurtosis\n",
    "\n",
    "\n",
    "* A parameter $\\mu = \\mu(P)$ is any function of the distribution P\n",
    "\n",
    "* Suppose $D_n = (x_1, x_2,..., x_n)$ is an i.i.d (independent and identically distributed) sample from P\n",
    "\n",
    "* A statistic $\\hat{\\mu} = \\hat{\\mu}(D_n)$ is a empirical estimator of $\\mu$ if $\\hat{\\mu} \\approx \\mu$\n",
    "\n",
    "  e.g. mean, median, variance, ...\n",
    "  \n",
    "Let $\\mu$ : P -> R be a real-valued **parameter**\n",
    "\n",
    "and $\\hat{\\mu}$ : $D_n$ ->  R be an empirical estimator of $\\mu$\n",
    "\n",
    "-> $\\mu = \\mu(P)$ and $\\hat{\\mu} = \\hat{\\mu}(D_n)$\n",
    "\n",
    "* **Bias**: Bias($\\hat{\\mu}$) = $\\mathbb{E}\\hat{\\mu} - \\mu$\n",
    "\n",
    "<b></b>\n",
    "\n",
    "\n",
    "\n",
    "* **Variance**: Var($\\hat{\\mu}$) = $\\mathbb{E}\\hat{\\mu}^2 - (\\mathbb{E}\\hat{\\mu})^2$\n",
    "\n",
    "<b></b>\n",
    "\n",
    "* Estimator unbiased if Bias($\\hat{\\mu}$) = $\\mathbb{E}\\hat{\\mu} - \\mu$ = 0\n",
    "\n",
    "\n",
    "### Averaging\n",
    "\n",
    "-> standard method to **reduce variance**\n",
    "\n",
    "-> increase prediction accuracy\n",
    "\n",
    "Idea: \n",
    "* divide training data set $D_n$ of size n in B smaller, independent groups $D_{n/B}^1, D_{n/B}^2, .... D_{n/B}^B$\n",
    "* build classifier for each individual training data set\n",
    "* average resulting predictions\n",
    "\n",
    "$$\\hat{f}_{avg}(x) = \\frac{1}{B} \\sum_{i=1}^{B} \\hat{f}_i(x)$$\n",
    "\n",
    "#### Bias of average estimator\n",
    "\n",
    "Bias does not increase when averaging over unbiased estimators\n",
    "\n",
    "Set of estimators $\\hat{f}_{1}(x), \\hat{f}_{2}(x), ... , \\hat{f}_{B}(x)$\n",
    "\n",
    "$$Bias[\\hat{f}_{avg}(x)]  = \\frac{1}{B} \\sum_{i=1}^{B} Bias[\\hat{f}_{i}(x)]$$\n",
    "\n",
    "#### Variance of average estimator\n",
    "\n",
    "Variance decreases when averaging over estimators\n",
    "\n",
    "Set of estimators $\\hat{f}_{1}(x), \\hat{f}_{2}(x), ... , \\hat{f}_{B}(x)$\n",
    "\n",
    "$$Var[\\hat{f}_{avg}(x)] = \\frac{1}{B^2} \\sum_{i=1}^{B} Var [\\hat{f}_i(x)] $$\n",
    "\n",
    "**But:** Predictions itself would not be as good as if we used all data $D_n$\n",
    "\n",
    "### Bagging - Bootstrapped data sets\n",
    "\n",
    "-> simulate having B independent training data sets of size n by drawing with replacement from $D_n$\n",
    "\n",
    "\n",
    "$$\\hat{f}_{bag}(x) = \\frac{1}{B} \\sum_{i=1}^{B} \\hat{f}_{*i}(x)$$\n",
    "\n",
    "In a bootstraped data set some elements\n",
    "* will show up multiple times\n",
    "* will not show up at all\n",
    "\n",
    "### Out-of-Bag error estimation\n",
    "\n",
    "observations that are not used by bootstrapped data set (out-of-bag) can be used to estimate test error\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "# Random forest\n",
    "\n",
    "-> improvement of bagging: *decorrelates* trees by using a random subset of features at each split\n",
    "\n",
    "Aggregation of many decision trees.\n",
    "\n",
    "Each tree has less predictive power than the ensemble of the trees.\n",
    "\n",
    "\n",
    "### Variable importance\n",
    "\n",
    "Randomly change values of features and evaluate decrease in prediction accuracy\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [],
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "# Define Random Forest Classifier and create model\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "def inter_rf(std, n_est):\n",
    "    clf_rf = RandomForestClassifier(n_estimators=n_est,max_features='auto', n_jobs=-1, min_samples_leaf = 1, \\\n",
    "                                    random_state=42, max_depth=None)\n",
    "\n",
    "\n",
    "    X, Y = make_blobs(n_samples=400, centers=3,\n",
    "                  random_state=0, cluster_std=std)\n",
    "    model = clf_rf.fit(X,Y)\n",
    "    visualize_classifier(model, X, Y)\n",
    "\n",
    "interact(inter_rf, n_est=(1,200), std=(0.0,2.0));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.close('all')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "# Example - Classifying seismic data of mass movements"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "Over recent years, seismology has evolved into a standard tool to study temporal and spatial variability of mass movement prone areas. Increasing data volumes and the demand for near real-time monitoring call for automated techniques to detect and classify seismic signals generated by mass movements.\n",
    "\n",
    "To classify different source mechanism of seismic signals, the signal has to be \"simplified\" in different ways to describe the waveform with one value. Such simplifications are the socalled features we have heard about before. \n",
    "For seismic signals, these can for example be the duration of the signal or the dominant frequencies.\n",
    "\n",
    "The study site we will be working on is Illgraben in the southwest of Switzerland. The Illgraben catchment is one of the most active mass wasting sites in the European Alps, with numerous rock-slope failure events and several debris flows per year. The Figure shows the Illgraben with the seismic network (ILL*) and the mass wasting deposition area in red."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "![Illgraben](figs/Illgraben_all.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "For the classification problem, we define four different classes: \n",
    "\n",
    "* Earthquake\n",
    "* Microquake\n",
    "* Rock avalanche\n",
    "* Noise\n",
    "\n",
    "An event catalog of four month in 2017 contains all events that were recorded on the stations. Here, we will treat each signal that was recorded as a single event. For instance, a local earthquake that was recorded on 5 stations will be treated as 5 different events.\n",
    "The event catalog was labeled and cross validated by experts and consists of starttime, endtime, duration, event type and the station it was recorded on."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example catalog - one event of each class recorded on several stations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "# Read catalog with four example events\n",
    "cat_ex = pd.read_csv('material/event_catalog2017_example.txt', delimiter=' ', skiprows=1, \\\n",
    "                  names=['sdate', 'stime', 'edate', 'etime', 'duration', 'Type', 'station'])\n",
    "\n",
    "# Add event names to pandas data frame - also called qualitative/categorical/discrete variables\n",
    "event_names = np.array(['earthquake', 'microquake', 'rockavalanche', 'noise'])\n",
    "tar = np.array(cat_ex['Type']) -1\n",
    "cat_ex['event'] = pd.Categorical.from_codes(tar, event_names)\n",
    "\n",
    "# Choose only events recorded on station ILL08\n",
    "cat08 = cat_ex[cat_ex['station'] == 'XP.ILL08..EHZ']\n",
    "cat08\n",
    "\n",
    "cat_ex.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [],
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "# Remove response\n",
    "def rere(st, inv):\n",
    "    #preprocessing\n",
    "    pre_filt = [0.005, 0.006, 45.0, 50.0]\n",
    "    #st.trim(ev_time, end_time)\n",
    "    st.remove_response(inventory = inv, pre_filt = pre_filt, \\\n",
    "            output = 'VEL', water_level = 60, plot=False)\n",
    "    return st\n",
    "\n",
    "# Plot waveform, spectra and spectrogram of event\n",
    "def plotwaveyspec(st,fmin, fmax):\n",
    "    tr = st[0]\n",
    "    # Setup figure\n",
    "    fig = plt.figure(figsize = (9, 5))\n",
    "    # [left bottom width height] \n",
    "    ax1 = fig.add_axes([0.25, 0.1, 0.65, 0.2]) # WAVEFORM\n",
    "    ax4 = fig.add_axes([0.1, 0.3, 0.15, 0.6]) # SPECTRUM\n",
    "    ax2 = fig.add_axes([0.25, 0.3, 0.65, 0.6], sharex=ax1, sharey=ax4) # SPECTROGRAM\n",
    "    ax3 = fig.add_axes([0.91, 0.3, 0.02, 0.6]) # COLORBAR\n",
    "\n",
    "    cmap = plt.get_cmap('viridis')\n",
    "    \n",
    "    # Plot waveforms (left subfigure)\n",
    "    #xt = tr.times()\n",
    "    ax1.plot(tr.times(), tr.data, color=cmap.colors[0], label=\"%s\" %tr.id)\n",
    "    ax1.set_xlabel(\"Time (s)\")\n",
    "    ax1.set_ylabel(\"Amplitude\")\n",
    "    ax1.set_xlim(0, tr.times()[-1])\n",
    "    ax1.legend(loc=1)\n",
    "\n",
    "    # Plot spectrogram (bottom right figure)\n",
    "    fig = tr.spectrogram(wlen=0.00015*len(tr.data), cmap=cmap, show=False, axes=ax2)\n",
    "    ax2.set_ylim(fmin, fmax)\n",
    "    ax2.set_xlim(0, tr.times()[-1])\n",
    "\n",
    "  \n",
    "    # Plot colorbar\n",
    "    mappable = ax2.images[0]\n",
    "    plt.colorbar(mappable=mappable, cax=ax3)\n",
    "\n",
    "    # Plot spectrum (top right figure)\n",
    "    sp, fr, line, = ax4.magnitude_spectrum(tr.data, tr.stats.sampling_rate, visible=False)\n",
    "    ax4.plot(sp,fr, color=cmap.colors[0])\n",
    "    ax4.set_xlabel('Magnitude')\n",
    "    ax4.set_xlim(min(sp), max(sp))\n",
    "    ax4.invert_xaxis()\n",
    "    ax4.set_ylabel('Frequency (Hz)')\n",
    "    \n",
    "    plt.setp(ax2.get_xticklabels(), visible=False)\n",
    "    plt.setp(ax2.get_yticklabels(), visible=False)\n",
    "    plt.setp(ax1.get_yticklabels(), visible=False)\n",
    "    plt.setp(ax4.get_xticklabels(), visible=False)\n",
    "    plt.setp(ax3.get_xticklabels(), visible=False)\n",
    "\n",
    "    \n",
    "    ax2.set_title(\"Station {} - {}\".format(tr.stats.station,tr.stats.starttime.ctime()))\n",
    "    #plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [],
    "scrolled": false,
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "# Import inventory\n",
    "inv = obspy.read_inventory('material/ILL1-8_inventory.xml')\n",
    "\n",
    "# Prepare and plot data\n",
    "for i in cat08.index.values:\n",
    "    starttime = UTCDateTime('%sT%s' % (cat_ex['sdate'][i], cat_ex['stime'][i]))\n",
    "    endtime = UTCDateTime('%sT%s' % (cat_ex['edate'][i], cat_ex['etime'][i]))\n",
    "    station = cat_ex['station'][i][3:8]\n",
    "    st = obspy.read(f'material/*{starttime.julday:03d}')\n",
    "    st.trim(starttime-10, endtime+10)\n",
    "    print(i, cat_ex['event'][i], st)\n",
    "    rere(st, inv)\n",
    "    st.detrend('linear')\n",
    "    st.detrend('demean')\n",
    "    st.taper(0.1)\n",
    "    f_min = 1\n",
    "    f_max = 30\n",
    "    st.filter('bandpass', freqmin = f_min, freqmax = f_max)\n",
    "    plotwaveyspec(st, f_min, f_max)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [],
    "scrolled": false,
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "# Calculate features for all traces in example catalog\n",
    "import ComputeAttributes_MW\n",
    "\n",
    "def attributes_all_traces(cat):\n",
    "    all_char = {}\n",
    "    for i in cat.index.values:\n",
    "        ev_type = cat['Type'][i]\n",
    "        starttime = UTCDateTime('%sT%s' % (cat['sdate'][i], cat['stime'][i]))\n",
    "        endtime = UTCDateTime('%sT%s' % (cat['edate'][i], cat['etime'][i]))\n",
    "        station = cat['station'][i][3:8]\n",
    "        st = obspy.read(f'material/*{starttime.julday:03d}')\n",
    "        st.trim(starttime, endtime)\n",
    "        st.detrend('demean')\n",
    "        print(i, st)\n",
    "        rere(st, inv) \n",
    "\n",
    "        for tr in st:\n",
    "            att = ComputeAttributes_MW.calculate_all_attributes(tr.data,tr.stats.sampling_rate)\n",
    "            type_att = np.append(ev_type, att)\n",
    "            all_char[i] = {'attributes': type_att, 'signal': st[0].data}\n",
    "\n",
    "\n",
    "    return all_char\n",
    "# Write attributes to dictionary with event class, attributes and signal of event\n",
    "\n",
    "char = attributes_all_traces(cat_ex)\n",
    "\n",
    "char[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [],
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "# Create array of event class and attributes\n",
    "att_arr = np.vstack([char[x]['attributes'] for x in list(char.keys())])\n",
    "\n",
    "# Save array to csv file\n",
    "np.savetxt('material/example_attributes.csv', att_arr, delimiter=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "# Read csv file with event class as first column and attributes as [1:8]\n",
    "columns = ['event_class','duration', 'RappMaxMean', 'AsDec', 'DistDecAmpEnv', 'env_max/duration(Data,sps)', 'MeanFFT', 'MaxFFT']\n",
    "\n",
    "# Read to data frame\n",
    "df = pd.read_csv('material/example_attributes.csv', names=columns,  dtype={'event_class':np.int32})\n",
    "df['event_class'] = df['event_class'] -1\n",
    "\n",
    "#Add event names\n",
    "event_names = np.array(['earthquake', 'microquake', 'rockavalanche', 'noise'])\n",
    "tar = np.array(df['event_class'])\n",
    "df['event'] = pd.Categorical.from_codes(tar, event_names)\n",
    "\n",
    "\n",
    "# Choose features to plot\n",
    "features = df[['duration', 'RappMaxMean', 'MeanFFT', 'event']] \n",
    "from matplotlib import rcParams\n",
    "\n",
    "# Make the pair plot of features with kernel density estiamtion on diagonal\n",
    "sns.pairplot(features, hue = 'event', diag_kind = 'kde', plot_kws=dict(alpha = 0.5),\n",
    "                   diag_kws=dict(shade=True), size=(4));\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Full event catalog with 58 calculated attributes\n",
    "after Provost, F., Hibert, C. and Malet, J.P., 2017. Automatic classification of endogenous landslide seismicity using the Random Forest supervised classifier. Geophysical Research Letters, 44(1), pp.113-120."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read csv file with event class as first column and attributes as [1:59]\n",
    "columns = ['event_class','duration', 'RappMaxMean','RappMaxMedian', 'AsDec', 'KurtoSig', \\\n",
    "                                               'KurtoEnv', 'SkewnessSig', 'SkewnessEnv', 'CorPeakNumber', 'INT1', \\\n",
    "                                               'INT2', 'INT_RATIO', 'ES[0]', 'ES[1]', 'ES[2]', 'ES[3]', 'ES[4]', 'KurtoF[0]', \\\n",
    "                                               'KurtoF[1]', 'KurtoF[2]', 'KurtoF[3]', 'KurtoF[4]', 'DistDecAmpEnv', \\\n",
    "                                               'env_max/duration(Data,sps)', 'MeanFFT', 'MaxFFT', 'FmaxFFT', \\\n",
    "                                               'FCentroid', 'Fquart1', 'Fquart3', 'MedianFFT', 'VarFFT', 'NpeakFFT', \\\n",
    "                                               'MeanPeaksFFT', 'E1FFT', 'E2FFT', 'E3FFT', 'E4FFT', 'gamma1', 'gamma2', \\\n",
    "                                               'gammas', 'SpecKurtoMaxEnv', 'SpecKurtoMedianEnv', 'RATIOENVSPECMAXMEAN', \\\n",
    "                                               'RATIOENVSPECMAXMEDIAN', 'DISTMAXMEAN', 'DISTMAXMEDIAN', 'NBRPEAKMAX', \\\n",
    "                                               'NBRPEAKMEAN', 'NBRPEAKMEDIAN', 'RATIONBRPEAKMAXMEAN', \\\n",
    "                                               'RATIONBRPEAKMAXMED', 'NBRPEAKFREQCENTER', 'NBRPEAKFREQMAX', \\\n",
    "                                               'RATIONBRFREQPEAKS', 'DISTQ2Q1', 'DISTQ3Q2', 'DISTQ3Q1']\n",
    "# Read to data frame\n",
    "df = pd.read_csv('material/all_attributes.csv', names=columns, dtype={'event_class':np.int32})\n",
    "df['event_class'] = df['event_class'] -1\n",
    "\n",
    "# Define actual names for events and append\n",
    "event_names = np.array(['earthquake', 'microquake', 'rockavalanche', 'noise'])\n",
    "tar = np.array(df['event_class'])\n",
    "df['event'] = pd.Categorical.from_codes(tar, event_names)\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose features to plot\n",
    "features = df[['duration', 'RappMaxMean', 'MeanFFT', 'event']] \n",
    "\n",
    "# Make the pair plot of features with kernel density estiamtion on diagonal\n",
    "sns.pairplot(features, hue = 'event', diag_kind = 'kde', plot_kws=dict(alpha = 0.5),\n",
    "                   diag_kws=dict(shade=True), size=(4));\n",
    "\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [Scikit-learn RandomForestClassifier](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define nb of training samples\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "test_sizes = [0.5, 0.4, 0.3, 0.1]\n",
    "for nb in test_sizes:\n",
    "    features = df.columns[1:58]\n",
    "    y = np.asarray(df['event_class'])\n",
    "    X = np.asarray(df[features])\n",
    "\n",
    "\n",
    "    # Seperate train and test data\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=nb, random_state=42)\n",
    "\n",
    "    # Create a random forest Classifier\n",
    "    clf = RandomForestClassifier(n_estimators=1400,criterion='gini',max_features='auto', \\\n",
    "                                 n_jobs=-1, min_samples_leaf = 1, max_depth=40, min_samples_split=2, \\\n",
    "                                 oob_score=True, random_state=None, bootstrap=True)\n",
    "\n",
    "    # Train the Classifier \n",
    "    clf.fit(X_train, y_train)\n",
    "\n",
    "        \n",
    "    # Show the number of observations for the test and training dataframes\n",
    "    print('Number of observations in the training data:', len(y_train))\n",
    "    #print('Number of observations in the test data:',len(y_test))\n",
    "    print(f'For {len(y_train)} training samples, the out-of-bag score is {clf.oob_score_}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_probs = clf.predict_proba(X_test)\n",
    "clf_probs[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "def plot_confusion_matrix(cm, classes,\n",
    "                          normalize=False,\n",
    "                          title='Confusion matrix',\n",
    "                          cmap=plt.cm.Blues):\n",
    "\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(8,8))\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title, fontsize=20)\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=45)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "    ax.tick_params(labelsize=15)\n",
    "    fmt = '.2f' if normalize else 'd'\n",
    "    thresh = cm.max() / 2.\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, format(cm[i, j], fmt),\n",
    "                 horizontalalignment=\"center\",\n",
    "                 color=\"white\" if cm[i, j] > thresh else \"black\", fontsize=15)\n",
    "    plt.ylabel('True label', fontsize=15)\n",
    "    plt.xlabel('Predicted label', fontsize=15)\n",
    "    plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "# Apply the Classifier we trained to the test data (which, remember, it has never seen before)\n",
    "pred = clf.predict(X_test)\n",
    "\n",
    "cnf_matrix = confusion_matrix(y_test, pred)\n",
    "np.set_printoptions(precision=2)\n",
    "\n",
    "# Plot normalized confusion matrix\n",
    "plt.figure()\n",
    "plot_confusion_matrix(cnf_matrix, classes=event_names, normalize=True,\n",
    "                      title='Normalized confusion matrix', cmap=plt.cm.bone_r)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# View a list of the features and their importance scores\n",
    "xx = sorted(zip(df[features], clf.feature_importances_), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "[(i[0], i[1] / xx[0][1]) for i in xx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     1
    ]
   },
   "outputs": [],
   "source": [
    "# Test model\n",
    "def evaluate(model, test_features, test_labels):\n",
    "    score = clf.score(test_features, test_labels)\n",
    "    print('Model Performance')\n",
    "    print('Score: {:0.4f}'.format(score))   \n",
    "    return score\n",
    "\n",
    "score = evaluate(clf, X_test, y_test)"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
